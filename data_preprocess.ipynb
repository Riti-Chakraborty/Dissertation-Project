{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify Emails beyond spams - Data Preprocessing\n",
    "\n",
    "### This notebook deals with data preprocessing. A csv file is exported which is used in the other files as input data. this fine need to get excecuted first as it generates the input file for other notebooks.\n",
    "\n",
    "### Final Project - Riti Chakraborty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Riti Chakraborty\n",
    "\n",
    "#importing the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import nan\n",
    "\n",
    "#For random seed\n",
    "import random\n",
    "\n",
    "#To handle warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "#For flattening lists\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#library for tf-idf vector and other similarities measure\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "\n",
    "#For using Regular expression\n",
    "import re\n",
    "\n",
    "#For Handling Strings\n",
    "import string\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reading the .csv file generated after "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Emails in the Dataset: 4035\n",
      "Total Number of Spams Available: 1241\n"
     ]
    }
   ],
   "source": [
    "#Reading Email_train Data.\n",
    "Emails = pd.read_csv('../input_data/TRAININGEMAILS.csv')\n",
    "\n",
    "#Adding the label column 0-Spam and 1- Ham\n",
    "label = pd.read_csv('../input_data/Label.txt', sep=\" \", header=None)\n",
    "Emails['label'] = label[0]\n",
    "\n",
    "\n",
    "#Keeping those records for which From Field in not Empty i.e. Removing Empty Emails.\n",
    "Emails_Notnull= Emails[pd.notnull(Emails['From'])]\n",
    "print(\"Total Number of Emails in the Dataset:\",Emails_Notnull.shape[0])\n",
    "\n",
    "#Seperatinbg out the spams\n",
    "Spams = Emails_Notnull.loc[Emails_Notnull['label']==0]\n",
    "Hams = Emails_Notnull.loc[Emails_Notnull['label']==1]\n",
    "Spams.to_csv('../exported_tables/Spams_1.csv')\n",
    "\n",
    "#Subsetting records with only 20% NA values \n",
    "Spams_set = Spams.loc[:, pd.notnull(Spams).sum()>len(Spams)*.8]\n",
    "print(\"Total Number of Spams Available:\",Spams_set.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming the required Columns\n",
    "Spams_set = Spams_set.rename(columns={'Message-id': 'MessageID'})\n",
    "\n",
    "#Renaming the columns Content type and MIME Version\n",
    "Spams_set = Spams_set.rename(columns={'Content-type': 'ContentType'})\n",
    "Spams_set = Spams_set.rename(columns={'Mime-version': 'MimeVersion'})\n",
    "\n",
    "\n",
    "#Seperating out values\n",
    "#Column: MessageID\n",
    "Spams_set['LocalPart'] = Spams_set.MessageID.str.split('@').str.get(0)\n",
    "Spams_set['Domain'] = Spams_set.MessageID.str.split('@').str.get(1)\n",
    "\n",
    "#Column: From\n",
    "Spams_set['From1'] = Spams_set.From.str.split('@').str.get(0)\n",
    "Spams_set['From2'] = Spams_set.From.str.split('@').str.get(1)    \n",
    "\n",
    "\n",
    "#Column: Received\n",
    "Spams_set['Receivedfrom'] = Spams_set.Received.str.split('by').str.get(0)\n",
    "Spams_set['Receivedby'] = Spams_set.Received.str.split('by').str.get(1)    \n",
    "\n",
    "\n",
    "#Column: Date\n",
    "Spams_set['Date']=pd.to_datetime(Spams_set['Date'], errors = 'coerce')\n",
    "\n",
    "#Column: ReturnPath\n",
    "Spams_set = Spams_set.rename(columns={'Return-path': 'Returnpath'})\n",
    "Spams_set['Returnpath1'] = Spams_set.Returnpath.str.split('@').str.get(0)\n",
    "Spams_set['Returnpath2'] = Spams_set.Returnpath.str.split('@').str.get(1)    \n",
    "\n",
    "#Column: To\n",
    "Spams_set['To1'] = Spams_set.To.str.split('@').str.get(0)\n",
    "Spams_set['To2'] = Spams_set.To.str.split('@').str.get(1)    \n",
    "\n",
    "#Column: Delivered To\n",
    "Spams_set = Spams_set.rename(columns={'Delivered-to': 'Deliveredto'})\n",
    "Spams_set['DeliveredTo1'] = Spams_set.Deliveredto.str.split('@').str.get(0)\n",
    "Spams_set['DeliveredTo2'] = Spams_set.Deliveredto.str.split('@').str.get(1)    \n",
    "\n",
    "#Extracting rows where date is not null\n",
    "Spams_set = Spams_set[pd.notnull(Spams_set['Date'])]\n",
    "\n",
    "\n",
    "#Extracting various different features from date\n",
    "Spams_set[\"Date\"]=pd.to_datetime(Spams_set[\"Date\"])\n",
    "Spams_set[\"Year\"]=Spams_set[\"Date\"].dt.year\n",
    "Spams_set[\"Month\"]=Spams_set[\"Date\"].dt.strftime(\"%B\")\n",
    "Spams_set[\"Day\"]=Spams_set[\"Date\"].dt.strftime(\"%A\")\n",
    "Spams_set[\"Hour\"]=Spams_set[\"Date\"].dt.hour\n",
    "\n",
    "Spams_set['Year'] = 'Year' + Spams_set['Year'].astype(str)\n",
    "Spams_set['Hour'] = 'Hour' + Spams_set['Hour'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rechecking for empty rows\n",
    "Spams_set.drop(Spams_set.query(\"Subject == ''| ContentType == ''|  MimeVersion == '' | From == '' | MessageID == '' | DeliveredTo1 == '' | DeliveredTo2 == '' | label == '' | LocalPart == '' | Domain == '' | From1 == '' | From2 == '' | Receivedfrom == '' | Receivedby == '' | Returnpath1 == '' | Returnpath2 == '' | To1 == '' | To2 == '' | DeliveredTo1 == '' | DeliveredTo2 == '' | Year == '' | Month == '' | Day == '' |  Hour == ''\").index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming IP address\n",
    "rec_column = Spams_set['Received'].tolist()\n",
    "ip_1=[]\n",
    "for i in rec_column:\n",
    "    ip =  re.findall(r'\\[(\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})\\]', i)\n",
    "    if len(ip) > 1:\n",
    "        ip_1.append([ip[0]])\n",
    "    else:\n",
    "        ip_1.append(ip)\n",
    "\n",
    "ip_1df=pd.DataFrame(ip_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reverse Geocoding -> Extracting the Location name from the IP address "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycountry\n",
    "pycountry.countries.get(alpha_2='CN').name\n",
    "import geocoder\n",
    "\n",
    "# country_ip=[]\n",
    "# for j in ip_1df[0].tolist():\n",
    "#     if j != None:\n",
    "#         g = geocoder.ip(j)\n",
    "#         country_ip.append(g.country)\n",
    "# #         print(g.country)\n",
    "#     else:\n",
    "#         country_ip.append(None)\n",
    "# pd.DataFrame(country_ip).to_csv(\"../exported_tables/country_ip.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_ip1=pd.read_csv(\"../exported_tables/country_ip.csv\")\n",
    "country_ip2=country_ip1['0'].tolist()\n",
    "country_ip=[str(s) for s in country_ip2] \n",
    "\n",
    "\n",
    "Loc_from_ip=[]\n",
    "for c_ip in country_ip:\n",
    "#     print(c_ip)\n",
    "    if c_ip != 'nan' and c_ip != 'EU':\n",
    "        Loc_from_ip.append(pycountry.countries.get(alpha_2=c_ip).name)\n",
    "    elif c_ip == 'EU':\n",
    "        Loc_from_ip.append(\"Europe\")\n",
    "    else:\n",
    "        Loc_from_ip.append(None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After Preliminary tests, attributes which didnot contribute in clustering were removed. The attributes mentioned below are the finally chosen ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subsetting the dataframe extracting the columns which are created to store seperated features.\n",
    "data_subset = Spams_set[[\"Subject\", \"From1\", \"From2\", \"Returnpath2\", \"Month\", \"Day\"]]\n",
    "loc_ip=pd.DataFrame(Loc_from_ip)\n",
    "loc_ip = loc_ip.rename(columns={0: 'Location'})\n",
    "\n",
    "data_subset['Location_1']=pd.Series(loc_ip['Location'])\n",
    "\n",
    "data_subset.shape\n",
    "data_subset.to_csv(\"../exported_tables/data_subset.csv\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
